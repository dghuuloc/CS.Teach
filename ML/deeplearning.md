# <p align="center"> Deep Learning </p>
---
## Most Used Activation Functions In Deep Learning
Activation functions introduce non-linearity, allowing the network to learn complex patterns.
* Sigmoid(Logistic) Activation Function
* Hyperbolic Tangent (tanh)
* Rectified Linear Unit (ReLU)
* Leaky ReLU
* Parametric ReLU (PReLU)
* Softmax Activation Function

## References
* [Most Used Activation Functions In Deep Learning](https://medium.com/@fraidoonomarzai99/most-used-activation-functions-in-deep-learning-47cda037a4c2)
