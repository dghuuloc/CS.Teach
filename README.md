# [CS.Teach]
---

## Books
**Top Publishers of Books on Computer Technologies That You Must Know:**
* Manning
* O’Reilly
* No Starch Press
* Packt
* Wiley
* Apress
* Pearson


```
>>> KEYNOTES
- Challenges & Limitations of Large Language Models
- Compression Algorithms for Large Language Models
- A Survey on Model Compression for Large Language Models
- compression multiple LLMs together ensemble models
- ensemble Models
- What is about one hot encoded
```

```
>>> RESEARCH
- Generative AI in healthcare or financial services
```

```txt
As you embark on learning about Large Language Models (LLMs), you might feel overwhelmed by the sheer amount of content available online. To ease this journey, I’ve compiled an overview of key topics in LLMs to help you grasp the concept in a structured way. Simply hearing about a new technology might not be enough to fully understand it, but breaking it down into digestible concepts and providing resources can be a great way to deepen your understanding.

In this post, I’ll share important resources and topics to explore, which will help you build a solid foundation in the world of LLMs. If a topic catches your interest, I encourage you to dive deeper into it using the provided links. Each video will guide you through a specific aspect of LLMs, ranging from the basics to more advanced topics.

Here’s an overview to get you started:

1. Introduction to Large Language Models (LLMs)
Get started with the basics of LLMs, what they are, and why they matter. Watch here

2. Pretraining vs. Fine-tuning LLMs
Learn the difference between pretraining and fine-tuning, two crucial steps in the development of LLMs. Watch here

3. What are Transformers?
Transformers are the backbone of many modern LLMs. Understand how this architecture works. Watch here

4. How Does GPT-3 Really Work?
Dive into the inner workings of one of the most well-known LLMs—GPT-3. Watch here

5. Stages of Building an LLM from Scratch
Explore the steps involved in building an LLM from the ground up. Watch here

6. Coding an LLM Tokenizer from Scratch in Python
A hands-on guide to understanding and building an LLM tokenizer. Watch here

7. The GPT Tokenizer: Byte Pair Encoding
Learn about one of the key techniques used in tokenization: Byte Pair Encoding (BPE). Watch here

8. What are Token Embeddings?
Understand the concept of token embeddings and their role in LLMs. Watch here

9. The Importance of Positional Embeddings
Explore how positional embeddings help LLMs understand the order of tokens in sequences. Watch here

10. The Data Preprocessing Pipeline of LLMs
Learn about the complex data preprocessing pipeline that powers LLMs. Watch here

By exploring these videos, you’ll gain a clearer understanding of how LLMs work and the various components that contribute to their success. I encourage you to follow these resources in the order that works best for you and dive deeper into topics that pique your interest.

If you have any questions or need further resources, feel free to ask! Happy learning


```


## Reference
* [Top 7 Publishers of Books on Computer Technologies That You Must Know](https://levelup.gitconnected.com/top-7-publishers-of-books-on-computer-technologies-that-you-must-know-b36d51e29bc1)
* [Compressing Large Language Models (LLMs)](https://towardsdatascience.com/compressing-large-language-models-llms-9f406eea5b5e/)
* [How to combine results from multiple retrievers](https://python.langchain.com/docs/how_to/ensemble_retriever/)
* [Compression Techniques for Large Language Models — Learn Together — Part 5](https://medium.com/@anilguven1055/compression-techniques-for-large-language-models-learn-together-part-5-8c9ae13b0c04)
* [DeltaZip: Efficient Serving of Multiple Full-Model-Tuned LLMs](https://dl.acm.org/doi/pdf/10.1145/3689031.3717468)
* [Prompt Compression in Large Language Models (LLMs): Making Every Token Count](https://medium.com/@sahin.samia/prompt-compression-in-large-language-models-llms-making-every-token-count-078a2d1c7e03)
* [LLMs == Compression](https://www.youtube.com/watch?v=2D2uRvW9A3w)
* [LLMZip: Lossless Text Compression using Large Language Models](https://openreview.net/forum?id=jhCzPwcVbG)
* [Is a llm just the most efficient compression algorithm we have ever created?](https://www.reddit.com/r/LocalLLaMA/comments/1cnpul3/is_a_llm_just_the_most_efficient_compression/)
* [Reviewing ‘Compression Algorithms for Large Language Models’ — shrinking the model size and reducing the cost of the hardware accelerators](https://www.linkedin.com/pulse/reviewing-compression-algorithms-large-language-models-basu-phd-mcyyc/)
* [Awesome-LLM-Compression](https://github.com/HuangOwen/Awesome-LLM-Compression)
* [A Comprehensive Survey of Compression Algorithms for Language Models](https://arxiv.org/pdf/2401.15347)
* [Understanding Compression of Large Language Models (LLMs)](https://medium.com/@sasirekharameshkumar/understanding-compression-of-large-language-models-2ee3b8a350a2)
* [Prompt Compression: A Guide With Python Example](https://www.datacamp.com/tutorial/prompt-compression)
* [Ensemble Models: What Are They and When Should You Use Them?](https://builtin.com/machine-learning/ensemble-model#:~:text=What%20Are%20Ensemble%20Models%3F,of%20building%20a%20single%20estimator.)
* [A Comprehensive Guide to Ensemble Learning (with Python codes)](https://www.analyticsvidhya.com/blog/2018/06/comprehensive-guide-for-ensemble-models/)
* [What is ensemble learning?](https://www.ibm.com/think/topics/ensemble-learning)
* [ensemble modeling](https://www.techtarget.com/searchbusinessanalytics/definition/Ensemble-modeling)
* [Ensemble Learning](https://www.geeksforgeeks.org/a-comprehensive-guide-to-ensemble-learning/)
* [Understanding Large Language Models (LLMs): A Comprehensive Overview](https://www.reddit.com/r/learnmachinelearning/comments/1h1awif/understanding_large_language_models_llms_a/)
* [Artificial Intelligence for Beginners - A Curriculum](https://github.com/microsoft/AI-For-Beginners)
* [Generative AI for Beginners](https://github.com/microsoft/generative-ai-for-beginners)
